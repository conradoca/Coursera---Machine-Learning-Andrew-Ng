{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c986c3d",
   "metadata": {},
   "source": [
    "# Ex6c - Email Features Extraction - Spam classification with SVM\n",
    "To use an SVM to classify emails into Spam v.s. Non-Spam, you first need to convert each email into a vector of features. This notebook shows the different steps used to extract these features from a given email.\n",
    "We will be using the NTLK (Natural Language Toolkit) for language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2c48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d988158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'emailSample1.txt'\n",
    "fh = open(fname)\n",
    "words = list()\n",
    "for line in fh:\n",
    "    linewords = line.split()\n",
    "    for word in linewords:\n",
    "        if word not in words:\n",
    "            words.append(word)\n",
    "# words.sort()\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9a4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a', 'web', 'portal', '?', 'Well,', 'depends', 'on', 'many', 'visitors', \"you're\", 'expecting.', 'This', 'can', 'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'month', 'couple', 'of', '$100.', 'You', 'should', 'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'Amazon', 'EC2', 'if', 'youre', 'running', 'something', 'big..', 'To', 'unsubscribe', 'yourself', 'this', 'mailing', 'list,', 'send', 'an', 'email', 'to:', 'groupname-unsubscribe@egroups.com']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2567b",
   "metadata": {},
   "source": [
    "## Email preprocess\n",
    "In this part, we will implement the preprocessing steps for each email producing a word indices vector for a given email. This involves:\n",
    "* Lower case\n",
    "* Remove any HTML markup\n",
    "* Replace all numbers with the text \"numbers\"\n",
    "* Replace all URLs with the test \"httpaddr\"\n",
    "* Replace dollar signal with dollar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f16da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a', 'web', 'portal', 'well', 'depends', 'on', 'many', 'visitors', 'youre', 'expecting', 'this', 'can', 'be', 'anywhere', 'from', 'less', 'than', 'number', 'bucks', 'month', 'couple', 'of', 'dollarnumber', 'you', 'should', 'checkout', 'httpaddr', 'or', 'perhaps', 'amazon', 'ecnumber', 'if', 'youre', 'running', 'something', 'big', 'to', 'unsubscribe', 'yourself', 'this', 'mailing', 'list', 'send', 'an', 'email', 'to', 'emailaddr']\n"
     ]
    }
   ],
   "source": [
    "# lower case\n",
    "words = list(map(lambda word: word.lower(), words))\n",
    "# Stripping HTML\n",
    "words = list(map(lambda word: re.sub('<[^<>]+>', ' ', word), words))\n",
    "# Handle numbers\n",
    "words = list(map(lambda word: re.sub('[0-9]+', 'number', word), words))\n",
    "# Handle URLs\n",
    "words = list(map(lambda word: re.sub('(http|https)://[^\\s]*', 'httpaddr', word), words))\n",
    "# Handle Email Addresses\n",
    "words = list(map(lambda word: re.sub('[^\\s]+@[^\\s]+', 'emailaddr', word), words))\n",
    "# Handle Email Addresses\n",
    "words = list(map(lambda word: re.sub('[$]+', 'dollar', word), words))\n",
    "# Remove any non alphanumeric characters\n",
    "words = list(map(lambda word: re.sub('[^a-zA-Z0-9]', '', word), words))\n",
    "# Remove any empty string\n",
    "words = list(filter(None, words))\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c4ce9",
   "metadata": {},
   "source": [
    "## Email tokenize\n",
    "We will be using the <b>PorterStemmer</b> algorithm to leave only the word's root (so 'running' becomes 'run')\n",
    "> This algorithm is part of the NLTK: https://www.nltk.org/. The O'Reilly book is in https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87c36e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a', 'web', 'portal', 'well', 'depend', 'on', 'mani', 'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from', 'less', 'than', 'number', 'buck', 'month', 'coupl', 'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or', 'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big', 'to', 'unsubscrib', 'yourself', 'thi', 'mail', 'list', 'send', 'an', 'email', 'to', 'emailaddr']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "words = list(map(lambda word: ps.stem(word), words))\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5547699",
   "metadata": {},
   "source": [
    "## Vocabulary list\n",
    "Now we'll be identifying which words we want to use on our filter and which we want to leave out. For this we will be using a <b>vocabulary list</b>, which consists of a list of words which occur at least 100 times in the spam corpus. The list we are using contains 1899 words, in practise these list range from 10.000 to 50.000 words.\n",
    "> <b>Purpose of the Vocabulary List:</b> Considering in a training set words that rarely occur may cause the model to overfit our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb86b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary list into a list\n",
    "# I was hesitating whether to use a list, a dictionary\n",
    "# or an array.\n",
    "# Dictionary and array would mantain an strong index-word relantionship\n",
    "# The list is an simpler structure but the word order with in the list\n",
    "# must be mantained. The 'sort' method would completely change the indexs \n",
    "fname = 'vocab.txt'\n",
    "fh = open(fname)\n",
    "vocabList = list()\n",
    "for line in fh:\n",
    "    linewords = line.split()\n",
    "    for word in linewords[1:]:\n",
    "        if word not in vocabList:\n",
    "            vocabList.append(word)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db5d4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 430, 1170, 1001, 1894, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 374, 1161, 478, 1892, 1509, 798, 1181, 1236, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n"
     ]
    }
   ],
   "source": [
    "# In this lambda we'll be using these two list functions:\n",
    "# 1) 'ab' in vocabList\n",
    "# 2) vocabList.index('patata')\n",
    "# With them we can create an elegant lambda that makes the \n",
    "# work in one line of code\n",
    "wordIndices = list(map(lambda word: vocabList.index(word) if (word in vocabList) else None, words))\n",
    "# Remove the characters not found\n",
    "wordIndices = list(filter(None, wordIndices))\n",
    "\n",
    "print(wordIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d10e7",
   "metadata": {},
   "source": [
    "## Extracting Features vector from Emails\n",
    "We will be coverting every email into a vector in $R^{n}$ where n is the number of words in our dictionary <code>vocabList</code>. \n",
    "$x_i=1$ is the i-th word is present and $x_i=0$ if that word is not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f909aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "featVector = np.zeros((len(vocabList)), dtype = int)\n",
    "for i in range(len(wordIndices)): featVector[wordIndices[i]]=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
